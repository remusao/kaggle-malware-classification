#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Usage:
    gen_train <archive> <labels.csv>
    gen_train -h | --help

Options:
    -h, --help      Show help
"""


from __future__ import print_function
import docopt
import utils
import features
import h5py


def load_train(path, labels):
    """ Lazily load data from archive. Iterate on (label, file, content) """
    for uid, content in utils.file_iter(path):
        print('UID', uid)
        if uid.endswith('.bytes'):
            uid = uid[:-6]
            yield (labels[uid], uid, content)


def load_labels(path):
    """ Load labels from CSV file, returns a mapping (file, label) """
    labels = {}
    with open(path, 'r') as labels_file:
        # Ignore first line (field names)
        next(labels_file)
        for line in labels_file:
            name, cls = line.split(',')
            name = name.strip('"')
            cls = int(cls)
            labels[name] = cls
    return labels


def main():
    args = docopt.docopt(__doc__)

    labels = load_labels(args['<labels.csv>'])
    train_set = load_train(args['<archive>'], labels)

    print("# Extract features")
    extractor = features.BinaryFeatureExtractor()
    train_features = extractor.fit_transform(train_set).todense()

    n_sample = train_features.shape[0]

    # Create dataset
    with h5py.File('train_set.hdf5', 'w', driver='core') as f:
        # X = training data
        print(train_features, train_features.shape)
        f.create_dataset(
            'X',
            dtype='f',
            compression="gzip",
            compression_opts=9,
            data=train_features)
        # Y = labels
        f.create_dataset(
            'Y',
            dtype='i',
            data=extractor.labels)

    # Dump features extractor
    utils.dump_classifier(extractor, 'feature_extractor.pkl')


if __name__ == "__main__":
    main()
